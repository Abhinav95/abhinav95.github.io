<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Abhinav Shukla</title>
  
  <meta name="author" content="Abhinav Shukla">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <!--<link rel="icon" type="image/png" href="images/seal_icon.png">-->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
	  
<!-- INTRODUCTION -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Abhinav Shukla</name>
              </p>
                	  <p>   I am a Researcher in Multimodal Machine Learning at <a href="https://scaledfoundations.ai/">Scaled Foundations</a>.
				My interests include self-supervised learning, embodied multimodal perception, large-scale multimodal (especially audiovisual) representation learning, and efficient multimodal inference.
		    	  </p>
			  <p>
				I was a Research Engineer at Meta from 2022 to 2023. Before that, I was a PhD student at <a href="https://ibug.doc.ic.ac.uk/">iBUG</a> (Intelligent Behaviour Understanding Group) at <a href="https://www.imperial.ac.uk">Imperial College London</a>
				where I was supervised by Prof. <a href ="https://ibug.doc.ic.ac.uk/maja/">Maja Pantic</a> and worked on self-supervised audiovisual representation learning and affective computing.
			 </p>
			  <p>
				I completed my Bachelors (Honours) and Masters by Research in Computer Science from <a href="https://www.iiit.ac.in/">IIIT Hyderabad</a> in 2017 and 2018 respectively.
	      </p>
              <p style="text-align:center">
                <a href="static/abhinav-shukla-cv.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=U1iLElwAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/Abhinav95_">Twitter</a> &nbsp/&nbsp
		<a href="https://www.linkedin.com/in/abhinavshukla95/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/Abhinav95">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <!--<a href="static/linkedinhead.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="static/linkedinhead.jpg" class="hoverZoomLink"></a>-->
	      <img width="100%" src="images/headshot.jpg"
                style="object-fit:cover; border-radius:50%;">
            </td>
          </tr>
        </tbody></table>
		
<!-- RECENT NEWS -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding: 20px; width:100%;vertical-align:middle">
              <heading>News</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-bottom: 10px;"><tbody>
          <tr>
            <td style="padding-left:25px;width:100%;vertical-align:middle">
	      <li><b>[May '24]</b> Recognized as an Outstanding Reviewer at CVPR 2024.</li>
	      <li><b>[Sep '23]</b> Started working as a Researcher in Multimodal Machine Learning at Scaled Foundations.</li>
	      <li><b>[May '23]</b> Recognized as an Outstanding Reviewer at CVPR 2023.</li>
	      <li><b>[Mar '23]</b> <a href="https://fkryan.github.io/saal">Paper</a> accepted at CVPR 2023.</li>
	      <li><b>[Mar '22]</b> Started working as a Research Engineer in the Reality Labs Research organization at Meta.</li>    
              <li><b>[Mar '21]</b> Paper accepted in IEEE Transactions on Affective Computing.</li>
              <li><b>[Sep '20]</b> Started working with Anurag Kumar at an internship in the Audio team at Facebook Reality Labs (FRL) Research.</li>
              <li><b>[Jul '20]</b> Presented my work on audiovisual self-supervised learning of speech representations at ICML 2020.</li>
            </td>
          </tr>
        </tbody> </table>
		

<!-- RESEARCH -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
              <p>
                I have worked on a variety of problems in multimodal machine learning (audio, video, text, EEG, eye tracking), with applications in representation learning
				(for tasks like egocentric and audiovisual scene understanding, speech recognition), computer vision, multimedia and affective computing.
              </p>
			  <p>
				For an updated and complete list of publications, see <a href="https://scholar.google.com/citations?user=U1iLElwAAAAJ">Google Scholar</a>
			  </p>
            </td>
          </tr>
        </tbody></table>
			
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

			<tr id="matmamba2024">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
					<table style="width: 100%;"> <tr>
						<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
							<papertitle>MatMamba: A Matryoshka State Space Model</papertitle>
							<br>
							<strong>Abhinav Shukla</strong>,
							<a href="https://www.saihv.com/">Sai Vemprala</a>,
							<a href="https://adityakusupati.github.io/">Aditya Kusupati</a>,
							<a href="https://scholar.google.com/citations?user=4D1n8scAAAAJ">Ashish Kapoor</a>
							<br>
							<em>arXiv, 2024</em>
							<br>
							[<a href="https://arxiv.org/pdf/2410.06718">pdf</a>]
							[<a href="https://github.com/ScaledFoundations/MatMamba">code</a>]
							[<a href="https://huggingface.co/collections/scaledfoundations/matmamba-670701480fa415dc2de60453">checkpoints</a>]
						</td>
					</tr> </table>
				</td>
			</tr>

			<tr id="grid2023">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>GRID: A Platform for General Robot Intelligence Development</papertitle>
					  <br>
					  <a href="https://www.saihv.com/">Sai Vemprala</a>,
					  <a href="https://scholar.google.com/citations?user=OpWNikcAAAAJ">Shuhang Chen</a>,
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://www.linkedin.com/in/dinesh-narayanan-9153a58/">Dinesh Narayanan</a>,
					  <a href ="https://scholar.google.com/citations?user=4D1n8scAAAAJ">Ashish Kapoor</a>
					  <br>
					  <em>arXiv, 2023</em>
					  <br>
					  [<a href="https://github.com/ScaledFoundations/GRID-playground">website</a>]
					  [<a href="https://arxiv.org/pdf/2310.00887">pdf</a>]
					  [<a href="bibs/grid2023.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
		
			<tr id="cvpr2023">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Egocentric Auditory Attention Localization in Conversations</papertitle>
					  <br>
					  <a href="https://fkryan.github.io/">Fiona Ryan</a>,
					  <a href="http://www.hao-jiang.net/">Hao Jiang</a>,
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://rehg.org/">James Matthew Rehg</a>,
					  <a href ="https://www.vamsiithapu.com/">Vamsi Krishna Ithapu</a>
					  <br>
					  <em>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023</em>
					  <br>
					  [<a href="https://fkryan.github.io/saal">demo</a>]
					  [<a href="https://arxiv.org/pdf/2303.16024">pdf</a>]
					  [<a href="bibs/cvpr2023.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
		
			<tr id="taffc2021">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Does Visual Self-Supervision Improve Learning of Speech Representations for Emotion Recognition?</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://ibug.doc.ic.ac.uk/people/spetridis">Stavros Petridis</a>,
					  <a href ="https://ibug.doc.ic.ac.uk/maja/">Maja Pantic</a>
					  <br>
					  <em>IEEE Transactions on Affective Computing, 2021</em>
					  <br>
					  [<a href="https://arxiv.org/pdf/2005.01400">pdf</a>]
					  [<a href="bibs/taffc2021.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
			
			<tr id="sasicml2020">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Learning Speech Representations from Raw Audio by Joint Audiovisual Self-Supervision</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://ibug.doc.ic.ac.uk/people/spetridis">Stavros Petridis</a>,
					  <a href ="https://ibug.doc.ic.ac.uk/maja/">Maja Pantic</a>
					  <br>
					  <em>ICML Workshop - Self-Supervision in Audio and Speech, 2020</em>
					  <br>
					  [<a href="https://arxiv.org/pdf/2007.04134">pdf</a>]
					  [<a href="bibs/sasicml2020.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
			
			<tr id="sightsoundcvpr2020">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Visual Self-Supervision by Facial Reconstruction for Speech Representation Learning</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://ibug.doc.ic.ac.uk/people/spetridis">Stavros Petridis</a>,
					  <a href ="https://ibug.doc.ic.ac.uk/maja/">Maja Pantic</a>
					  <br>
					  <em>CVPR Workshop - Sight and Sound, 2020</em>
					  <br>
					  [<a href="http://sightsound.org/papers/2020/Abhinav_Shukla_Visual_Self-Supervision_by_Facial_Reconstruction_for_Speech_Representation_Learning.pdf">pdf</a>]
					  [<a href="bibs/sightsoundcvpr2020.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
			
			<tr id="icassp2020">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Visually Guided Self Supervised Learning of Speech Representations</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://ibug.doc.ic.ac.uk/people/kvougioukas">Konstantinos Vougioukas</a>,
					  <a href="https://mpc001.github.io/">Pingchuan Ma</a>,
					  <a href="https://ibug.doc.ic.ac.uk/people/spetridis">Stavros Petridis</a>,
					  <a href ="https://ibug.doc.ic.ac.uk/maja/">Maja Pantic</a>
					  <br>
					  <em>International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020, (Oral)</em>
					  <br>
					  [<a href="https://arxiv.org/pdf/2001.04316">pdf</a>]
					  [<a href="bibs/icassp2020.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
			
			<tr id="acmmm2020">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Learning Self-Supervised Multimodal Representations of Human Behaviour</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>
					  <br>
					  <em>Doctoral Symposium at ACM International Conference on Multimedia (ACM MM), 2020</em>
					  <br>
					  [<a href="https://dl.acm.org/doi/abs/10.1145/3394171.3416518">pdf</a>]
					  [<a href="bibs/acmmm2020.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
			
			<tr id="taffc2020">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Recognition of Advertisement Emotions with Application to Computational Advertising</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://gshruti95.github.io/">Shruti Shriya Gullapuram</a>,
					  <a href ="https://sites.google.com/view/harish-katti/home">Harish Katti</a>,
					  <a href ="https://www.comp.nus.edu.sg/~mohan/">Mohan Kankanhalli</a>,
					  <a href ="https://sites.google.com/site/raamsubram/">Ramanathan Subramanian</a>
					  <br>
					  <em>IEEE Transactions on Affective Computing, 2020</em>
					  <br>
					  [<a href="https://arxiv.org/pdf/1904.01778">pdf</a>]
					  [<a href="bibs/taffc2020.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
			
			<tr id="icmi2018">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Looking Beyond a Clever Narrative: Visual Context and Attention are Primary Drivers of Affect in Video Advertisements</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>,
					  <a href ="https://sites.google.com/view/harish-katti/home">Harish Katti</a>,
					  <a href ="https://www.comp.nus.edu.sg/~mohan/">Mohan Kankanhalli</a>,
					  <a href ="https://sites.google.com/site/raamsubram/">Ramanathan Subramanian</a>
					  <br>
					  <em>ACM International Conference on Multimodal Interaction (ICMI), 2018, (Oral, 15.4% acceptance rate)</em>
					  <br>
					  [<a href="https://arxiv.org/pdf/1808.04610">pdf</a>]
					  [<a href="bibs/icmi2018.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
			
			<tr id="icmi2017">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td class="underline" style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Evaluating Content-Centric vs. User-Centric Ad Affect Recognition</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://gshruti95.github.io/">Shruti Shriya Gullapuram</a>,
					  <a href ="https://sites.google.com/view/harish-katti/home">Harish Katti</a>,
					  <a href ="https://www.tudelft.nl/en/eemcs/the-faculty/departments/intelligent-systems/multimedia-computing/people/karthik-yadati/">Karthik Yadati</a>,
					  <a href ="https://www.comp.nus.edu.sg/~mohan/">Mohan Kankanhalli</a>,
					  <a href ="https://sites.google.com/site/raamsubram/">Ramanathan Subramanian</a>
					  <br>
					  <em>ACM International Conference on Multimodal Interaction (ICMI), 2017</em>
					  <br>
					  [<a href="https://arxiv.org/pdf/1709.01684">pdf</a>]
					  [<a href="bibs/icmi2017.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>
			
			<tr id="acmmm2017">
				<td style="padding-left:20px;width:100%;vertical-align:middle">
				  <table style="width: 100%;"> <tr>
					<td style="width: 80%;padding-bottom: 8px; padding-top: 8px;">
					  <papertitle>Affect Recognition in Ads with Application to Computational Advertising</papertitle>
					  <br>
					  <strong>Abhinav Shukla</strong>, 
					  <a href="https://gshruti95.github.io/">Shruti Shriya Gullapuram</a>,
					  <a href ="https://sites.google.com/view/harish-katti/home">Harish Katti</a>,
					  <a href ="https://www.tudelft.nl/en/eemcs/the-faculty/departments/intelligent-systems/multimedia-computing/people/karthik-yadati/">Karthik Yadati</a>,
					  <a href ="https://www.comp.nus.edu.sg/~mohan/">Mohan Kankanhalli</a>,
					  <a href ="https://sites.google.com/site/raamsubram/">Ramanathan Subramanian</a>
					  <br>
					  <em>ACM International Conference on Multimedia (ACM MM), 2017, (Oral, 7.5% acceptance rate)</em>
					  <br>
					  [<a href="https://arxiv.org/pdf/1709.01683">pdf</a>]
					  [<a href="bibs/acmmm2017.txt">bib</a>]
					</td>
				  </tr> </table>
				</td>
			</tr>


        </tbody></table>
		
<!-- EXPERIENCE -->
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;cellpadding:10px;"><tbody>

	<tr>
            <td style="padding-left:20px; width: 20%; vertical-align:middle">
              
              <table style="width: 100%; table-layout: fixed;"> <tr>
                <td style="padding: 0px; height: 80px; width: 20%;">
                  <img style="vertical-align: middle; width: 80%; margin-left: auto; margin-right: auto; display: block;" src="images/scaledfoundations.png">
                </td>
                <td style="width: 80%; padding-bottom: 8px; padding-top: 8px;">
                  <papertitle>Scaled Foundations</papertitle>
                  <br>
                  Researcher, Multimodal Machine Learning &nbsp;
                  <br>
                  Redmond, WA &nbsp; · &nbsp; <em> 2023 - present </em>
                  <br>
                  <p>
			  Working on large-scale multimodal representation learning.
		</p>
                </td> 
              </tr> </table>
			  
            
            </td>
          </tr>
			
          <tr>
            <td style="padding-left:20px; width: 20%; vertical-align:middle">
              
              <table style="width: 100%; table-layout: fixed;"> <tr>
                <td style="padding: 0px; height: 80px; width: 20%;">
                  <img style="vertical-align: middle; width: 80%; margin-left: auto; margin-right: auto; display: block;" src="images/meta.png">
                </td>
                <td style="width: 80%; padding-bottom: 8px; padding-top: 8px;">
                  <papertitle>Meta</papertitle>
                  <br>
                  Research Engineer &nbsp;
                  <br>
                  Redmond, WA &nbsp; · &nbsp; <em> 2022 - 2023 </em>
                  <br>
                  <p>
			  Worked in the Audio team at Reality Labs Research.
			  Developed infrastructure for large scale training, benchmarking, and optimizing multimodal/audiovisual models for AR glasses.
			  Conducted research on egocentric audiovisual machine learning and computer vision. Published at CVPR 2023.
		</p>
                </td> 
              </tr> </table>
			  
            
            </td>
          </tr>
		  
		  
		  
		</tbody></table>
		
<!-- INTERNSHIPS -->
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Internships</heading>
            </td>
          </tr>
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;cellpadding:10px;"><tbody>

          <tr>
            <td style="padding-left:20px; width: 20%; vertical-align:middle">
              
              <table style="width: 100%; table-layout: fixed;"> <tr>
                <td class="underline" style="padding: 0px; height: 80px; width: 20%;">
                  <img style="vertical-align: middle; width: 80%; margin-left: auto; margin-right: auto; display: block;" src="images/frl.png">
                </td>
                <td class="underline" style="width: 80%; padding-bottom: 8px; padding-top: 8px;">
                  <papertitle>Facebook Reality Labs (FRL) Research</papertitle>
                  <br>
                  Research Intern &nbsp; <em>with</em> &nbsp; <a href="https://anuragkr90.github.io/">Anurag Kumar</a>
                  <br>
                  Redmond, WA (Remote) &nbsp; · &nbsp; <em> Sep 2020 - May 2021 </em>
                  <br>
                  <p>
					Worked on visually guided self-supervised learning of audio representations.
				  </p>
                </td> 
              </tr> </table>
			  
			  <table style="width: 100%; table-layout: fixed;"> <tr>
                <td class="underline" style="padding: 0px; height: 80px; width: 20%;">
                  <img style="vertical-align: middle; width: 70%; margin-left: auto; margin-right: auto; display: block;" src="images/imperial.png">
                </td>
                <td class="underline" style="width: 80%; padding-bottom: 8px; padding-top: 8px;">
                  <papertitle>Imperial College London</papertitle>
                  <br>
                  Research Assistant &nbsp; <em>with</em> &nbsp; Prof. <a href ="https://ibug.doc.ic.ac.uk/maja/">Maja Pantic</a>
                  <br>
                  London, United Kingdom &nbsp; · &nbsp; <em> Oct 2018 - March 2019 </em>
                  <br>
                  <p>
					Worked as a research assistant in the iBUG group funded by the EU Horizon 2020 <a href="https://de-enigma.eu/">DE-ENIGMA</a> project.
					Assisted in collecting data of autistic children interacting with a social robot. Performed research for learning speech representations for emotion recognition.
				  </p>
                </td> 
              </tr> </table>
			  
			  <table style="width: 100%; table-layout: fixed;"> <tr>
                <td class="underline" style="padding: 0px; height: 80px; width: 20%;">
                  <img style="vertical-align: middle; width: 70%; margin-left: auto; margin-right: auto; display: block;" src="images/nus.jpg">
                </td>
                <td class="underline" style="width: 80%; padding-bottom: 8px; padding-top: 8px;">
                  <papertitle>National University of Singapore</papertitle>
                  <br>
                  Research Intern &nbsp; <em>with</em> &nbsp; Prof. <a href ="https://www.comp.nus.edu.sg/~mohan/">Mohan Kankanhalli</a>
                  <br>
                  Singapore &nbsp; · &nbsp; <em> Sep 2017 - May 2018 </em>
                  <br>
                  <p>
					Worked on multimodal (audio, video, EEG, eye tracking) affect recognition from advertisement videos at the SeSaMe (Sensor Enhanced Social Media) Centre.
					Published in IEEE Transactions on Affective Computing and at ICMI 2018.
				  </p>
                </td> 
              </tr> </table>
			  
			  <table style="width: 100%; table-layout: fixed;"> <tr>
                <td style="padding: 0px; height: 80px; width: 20%;">
                  <img style="vertical-align: middle; width: 80%; margin-left: auto; margin-right: auto; display: block;" src="images/gsoc.png">
                </td>
                <td style="width: 80%; padding-bottom: 8px; padding-top: 8px;">
                  <papertitle>Google Summer of Code</papertitle>
                  <br>
                  Student Developer &nbsp;
                  <br>
                  Remote &nbsp; · &nbsp; <em> Summer 2016 and Summer 2017</em>
                  <br>
                  <p>
					2017: Worked with Prof. <a href="http://cogweb.ucla.edu/steen/">Francis Steen</a> from UCLA and Prof. <a href="http://markturner.org/">Mark Turner</a> from CWRU for the Red Hen Lab organization. <br>
					2016: Developed a system to extract burned-in subtitles from videos into caption files for the CCExtractor organization.
					Supervised by <a href="https://www.linkedin.com/in/carlossubs/">Carlos Fernandez</a> (org admin and CEO of Subtix Inc).
				  </p>
                </td> 
              </tr> </table>
            
            </td>
          </tr>
		  
		  
		  
		</tbody></table>
		
<!-- EDUCATION -->
		<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education</heading>
            </td>
          </tr>
        </tbody></table>
		
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;cellpadding:10px;"><tbody>

          <tr>
            <td style="padding-left:20px; width: 20%; vertical-align:middle">
			  
			  <table style="width: 100%; table-layout: fixed;"> <tr>
                <td style="padding: 0px; height: 80px; width: 20%;">
                  <img style="vertical-align: middle; width: 70%; margin-left: auto; margin-right: auto; display: block;" src="images/imperial.png">
                </td>
                <td style="width: 80%; padding-bottom: 8px; padding-top: 8px;">
                  <papertitle>Imperial College London</papertitle>
                  <br>
                  PhD in Computer Science &nbsp;
                  <br>
                  London, United Kingdom &nbsp; · &nbsp; <em> 2018 - 2022 </em>
                  <br>
			Thesis title: "Learning Self-Supervised Representations of Audiovisual Human-Centric Data"
		  <br>
                </td> 
              </tr> </table>
            
			<table style="width: 100%; table-layout: fixed;"> <tr>
                <td style="padding: 0px; height: 80px; width: 20%;">
                  <img style="vertical-align: middle; width: 80%; margin-left: auto; margin-right: auto; display: block;" src="images/iiit.png">
                </td>
                <td  style="width: 80%; padding-bottom: 8px; padding-top: 8px;">
                  <papertitle>IIIT Hyderabad</papertitle>
                  <br>
                  BTech & MS by Research in Computer Science&nbsp; · &nbsp; <em> 8.78/10.00 </em>
                  <br>
                  Hyderabad, India &nbsp; · &nbsp; <em> 2013-2018 </em>
                  <br>
			I finished my thesis, "Multimodal Emotion Recognition from Advertisements with Application to Computational Advertising"
			advised by Prof. <a href="https://sites.google.com/site/raamsubram/">Ramanathan Subramanian</a> at the <a href="http://cvit.iiit.ac.in/">Center for Visual Information Technology</a>.
                  <br>
		</td> 
              </tr> </table>
			
            </td>
          </tr>
		</tbody></table>

<!-- AWARDS -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Awards and Recognition</heading>
            </td>
          </tr>
        </tbody></table>
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-bottom: 10px;"><tbody>
          <tr>
            <td style="padding-left:25px;width:100%;vertical-align:middle">
              <li><b>Samsung PhD Fellowship </b>, 2019-2020</li>
              <li><b>IIIT Hyderabad Fast-track Masters thesis</b> (for high quality papers in reputed venues), 2018</li>
			  <li><b>IIIT Hyderabad Research award </b>(for publishing as an undergraduate), 2018</li>
			  <li><b>ACM SIGCHI Gary Marsden Student Development Fund </b>(to attend ICMI 2018), 2018</li>
			  <li><b>Google India Travel Grant</b> (to attend ACM MM 2017), 2017</li>
			  <li><b>ACM ICMI 2017 Travel Grant</b> (to attend ICMI 2017), 2017</li>
			  <li><b>Dean's Merit List Award</b> for excellence in academics (6 consecutive semesters), 2014-2017</li>
            </td>
          </tr>
        </tbody> </table>
		
<!-- ACADEMIC SERVICE -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Academic Service</heading>
            </td>
          </tr>
        </tbody></table>
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto; padding-bottom: 10px;"><tbody>
          <tr>
            <td style="padding-left:25px;width:100%;vertical-align:middle">
	      <li><b>Conference Reviewing:</b> CVPR, ICCV, WACV, ICASSP, FG, ICMI, ACII</li>
              <li><b>Journal Reviewing:</b> IEEE Transactions on Affective Computing (TAFFC), IJCV, TPAMI</li>
              <li><b>Volunteer:</b> ACII 2019, ICMI 2018, ICMI 2017</li>
            </td>
          </tr>
        </tbody> </table>
        
		
<!-- FOOTER TEXT -->
		<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Last updated: November 2024. Template by <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>, with some sections from <a href="https://www.noveens.com/">Noveen Sachdeva</a>'s website.
                </p>
            </td>
          </tr>
        </tbody></table>
      
	  </td>
    </tr>
  </table>
</body>

</html>
